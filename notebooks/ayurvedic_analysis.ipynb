{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7e6e6d4",
   "metadata": {},
   "source": [
    "# Ayurvedic Medicine Sensor Analysis Pipeline\n",
    "\n",
    "This notebook demonstrates the complete machine learning pipeline for analyzing NIR sensor data from Ayurvedic medicines. We'll cover:\n",
    "\n",
    "1. Data loading and validation\n",
    "2. Exploratory data analysis\n",
    "3. Feature engineering\n",
    "4. Model development and training\n",
    "5. Evaluation and visualization\n",
    "6. Production deployment\n",
    "\n",
    "## Setup Requirements\n",
    "- Python 3.8+\n",
    "- Required packages: pandas, numpy, scikit-learn, tensorflow, plotly, fastapi\n",
    "- Git repository for version control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29d34cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.svm import SVR, SVC\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import joblib\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Import custom modules\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.data.preprocessing import DataPreprocessor\n",
    "from src.features.engineering import FeatureEngineer\n",
    "from src.models.ensemble import EnsembleModel\n",
    "from src.visualization.plots import Visualizer\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedf92a1",
   "metadata": {},
   "source": [
    "# Data Loading and Validation\n",
    "\n",
    "Let's create a sample dataset and demonstrate the data loading and validation pipeline. Our dataset should have the following structure:\n",
    "- 6 NIR wavelength sensors (R,S,T,U,V,W)\n",
    "- Temperature readings\n",
    "- Dilution percentages (100%, 75%, 50%, 25%, 10%)\n",
    "- Medicine names (3-4 different medicines)\n",
    "- Effectiveness scores\n",
    "- Reading IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da1e6c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data generated and saved to '../data/sensor_readings.csv'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R</th>\n",
       "      <th>S</th>\n",
       "      <th>T</th>\n",
       "      <th>U</th>\n",
       "      <th>V</th>\n",
       "      <th>W</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Dilution_Percent</th>\n",
       "      <th>Medicine_Name</th>\n",
       "      <th>Effectiveness_Score</th>\n",
       "      <th>Reading_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.192114</td>\n",
       "      <td>4.723471</td>\n",
       "      <td>6.295377</td>\n",
       "      <td>8.046060</td>\n",
       "      <td>4.531693</td>\n",
       "      <td>4.531726</td>\n",
       "      <td>28.158426</td>\n",
       "      <td>100</td>\n",
       "      <td>Ashwagandha</td>\n",
       "      <td>1.076743</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.873261</td>\n",
       "      <td>6.085120</td>\n",
       "      <td>4.073165</td>\n",
       "      <td>4.068540</td>\n",
       "      <td>5.483925</td>\n",
       "      <td>1.173440</td>\n",
       "      <td>21.550164</td>\n",
       "      <td>100</td>\n",
       "      <td>Ashwagandha</td>\n",
       "      <td>0.943771</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.569205</td>\n",
       "      <td>5.628495</td>\n",
       "      <td>3.183952</td>\n",
       "      <td>2.175393</td>\n",
       "      <td>7.931298</td>\n",
       "      <td>4.548447</td>\n",
       "      <td>25.135056</td>\n",
       "      <td>100</td>\n",
       "      <td>Ashwagandha</td>\n",
       "      <td>0.857525</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.693481</td>\n",
       "      <td>5.221845</td>\n",
       "      <td>2.698013</td>\n",
       "      <td>5.751396</td>\n",
       "      <td>3.798723</td>\n",
       "      <td>4.416613</td>\n",
       "      <td>23.796587</td>\n",
       "      <td>100</td>\n",
       "      <td>Ashwagandha</td>\n",
       "      <td>1.185228</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.967607</td>\n",
       "      <td>2.884578</td>\n",
       "      <td>6.645090</td>\n",
       "      <td>2.558313</td>\n",
       "      <td>5.417727</td>\n",
       "      <td>1.080660</td>\n",
       "      <td>22.343628</td>\n",
       "      <td>100</td>\n",
       "      <td>Ashwagandha</td>\n",
       "      <td>1.019686</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          R         S         T         U         V         W  Temperature  \\\n",
       "0  7.192114  4.723471  6.295377  8.046060  4.531693  4.531726    28.158426   \n",
       "1  4.873261  6.085120  4.073165  4.068540  5.483925  1.173440    21.550164   \n",
       "2  3.569205  5.628495  3.183952  2.175393  7.931298  4.548447    25.135056   \n",
       "3  4.693481  5.221845  2.698013  5.751396  3.798723  4.416613    23.796587   \n",
       "4  5.967607  2.884578  6.645090  2.558313  5.417727  1.080660    22.343628   \n",
       "\n",
       "   Dilution_Percent Medicine_Name  Effectiveness_Score  Reading_ID  \n",
       "0               100   Ashwagandha             1.076743           1  \n",
       "1               100   Ashwagandha             0.943771           2  \n",
       "2               100   Ashwagandha             0.857525           3  \n",
       "3               100   Ashwagandha             1.185228           4  \n",
       "4               100   Ashwagandha             1.019686           5  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate sample data\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define parameters\n",
    "n_samples_per_dilution = 25\n",
    "dilution_levels = [100, 75, 50, 25, 10]\n",
    "medicines = ['Ashwagandha', 'Turmeric', 'Tulsi', 'Neem']\n",
    "n_total_samples = n_samples_per_dilution * len(dilution_levels) * len(medicines)\n",
    "\n",
    "# Generate synthetic data\n",
    "data = []\n",
    "reading_id = 1\n",
    "\n",
    "for medicine in medicines:\n",
    "    for dilution in dilution_levels:\n",
    "        for _ in range(n_samples_per_dilution):\n",
    "            # Base sensor readings (higher for higher dilutions)\n",
    "            base_readings = np.random.normal(dilution/20, 2, 6)\n",
    "            \n",
    "            # Add medicine-specific patterns\n",
    "            if medicine == 'Ashwagandha':\n",
    "                base_readings[0] *= 1.2  # Stronger R response\n",
    "            elif medicine == 'Turmeric':\n",
    "                base_readings[2] *= 1.3  # Stronger T response\n",
    "            elif medicine == 'Tulsi':\n",
    "                base_readings[4] *= 1.4  # Stronger V response\n",
    "            else:  # Neem\n",
    "                base_readings[5] *= 1.5  # Stronger W response\n",
    "            \n",
    "            # Temperature variation\n",
    "            temperature = np.random.normal(25, 2)\n",
    "            \n",
    "            # Effectiveness score (correlated with dilution and some noise)\n",
    "            effectiveness = (dilution/100) * np.random.normal(1, 0.1)\n",
    "            \n",
    "            # Ensure non-negative values\n",
    "            base_readings = np.maximum(base_readings, 0)\n",
    "            \n",
    "            data.append({\n",
    "                'R': base_readings[0],\n",
    "                'S': base_readings[1],\n",
    "                'T': base_readings[2],\n",
    "                'U': base_readings[3],\n",
    "                'V': base_readings[4],\n",
    "                'W': base_readings[5],\n",
    "                'Temperature': temperature,\n",
    "                'Dilution_Percent': dilution,\n",
    "                'Medicine_Name': medicine,\n",
    "                'Effectiveness_Score': effectiveness,\n",
    "                'Reading_ID': reading_id\n",
    "            })\n",
    "            reading_id += 1\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save to CSV\n",
    "os.makedirs('../data', exist_ok=True)\n",
    "df.to_csv('../data/sensor_readings.csv', index=False)\n",
    "print(\"Sample data generated and saved to '../data/sensor_readings.csv'\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff92e82d",
   "metadata": {},
   "source": [
    "# Data Preprocessing and Feature Engineering\n",
    "\n",
    "Now let's preprocess the data and create engineered features using our custom modules. We'll:\n",
    "1. Initialize the data preprocessor\n",
    "2. Load and validate the data\n",
    "3. Apply temperature compensation\n",
    "4. Normalize features\n",
    "5. Engineer additional features using wavelength ratios and spectral derivatives\n",
    "6. Apply PCA for dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed9403de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original features shape: (500, 7)\n",
      "Engineered features shape: (500, 39)\n",
      "PCA features shape: (500, 3)\n"
     ]
    }
   ],
   "source": [
    "# Initialize processors\n",
    "preprocessor = DataPreprocessor()\n",
    "feature_engineer = FeatureEngineer()\n",
    "\n",
    "# Load and preprocess data\n",
    "df = preprocessor.load_data('../data/sensor_readings.csv')\n",
    "df_comp = preprocessor.temperature_compensation(df)\n",
    "df_norm = preprocessor.normalize_features(df_comp)\n",
    "\n",
    "# Prepare feature matrix\n",
    "X = np.hstack([\n",
    "    df_norm[preprocessor.sensor_columns].values,\n",
    "    df_norm[[preprocessor.temp_column]].values\n",
    "])\n",
    "\n",
    "# Engineer features\n",
    "features = feature_engineer.engineer_features(\n",
    "    df_norm[preprocessor.sensor_columns].values,\n",
    "    df_norm[preprocessor.temp_column].values\n",
    ")\n",
    "\n",
    "print(\"Original features shape:\", X.shape)\n",
    "print(\"Engineered features shape:\", features['combined'].shape)\n",
    "print(\"PCA features shape:\", features['pca'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e9c7af",
   "metadata": {},
   "source": [
    "# Data Visualization\n",
    "\n",
    "Let's create visualizations to understand our data better:\n",
    "1. Sensor reading distributions\n",
    "2. Temperature vs sensor response\n",
    "3. Dilution level effects\n",
    "4. Medicine type patterns\n",
    "5. Feature correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb4727e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Mime type rendering requires nbformat>=4.2.0 but it is not installed",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Plot sensor readings over time\u001b[39;00m\n\u001b[32m      5\u001b[39m fig_readings = visualizer.plot_sensor_readings(df)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mfig_readings\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Plot dilution curves\u001b[39;00m\n\u001b[32m      9\u001b[39m fig_dilution = visualizer.plot_dilution_curves(df)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\plotly\\basedatatypes.py:3420\u001b[39m, in \u001b[36mBaseFigure.show\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   3387\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3388\u001b[39m \u001b[33;03mShow a figure using either the default renderer(s) or the renderer(s)\u001b[39;00m\n\u001b[32m   3389\u001b[39m \u001b[33;03mspecified by the renderer argument\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3416\u001b[39m \u001b[33;03mNone\u001b[39;00m\n\u001b[32m   3417\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3418\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplotly\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpio\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3420\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\plotly\\io\\_renderers.py:415\u001b[39m, in \u001b[36mshow\u001b[39m\u001b[34m(fig, renderer, validate, **kwargs)\u001b[39m\n\u001b[32m    410\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    411\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMime type rendering requires ipython but it is not installed\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    412\u001b[39m     )\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nbformat \u001b[38;5;129;01mor\u001b[39;00m Version(nbformat.__version__) < Version(\u001b[33m\"\u001b[39m\u001b[33m4.2.0\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    416\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMime type rendering requires nbformat>=4.2.0 but it is not installed\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    417\u001b[39m     )\n\u001b[32m    419\u001b[39m display_jupyter_version_warnings()\n\u001b[32m    421\u001b[39m ipython_display.display(bundle, raw=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mValueError\u001b[39m: Mime type rendering requires nbformat>=4.2.0 but it is not installed"
     ]
    }
   ],
   "source": [
    "# Visualization of sensor readings\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Create subplots for all sensor readings\n",
    "fig = make_subplots(rows=2, cols=3, \n",
    "                    subplot_titles=('R Channel', 'S Channel', 'T Channel',\n",
    "                                  'U Channel', 'V Channel', 'W Channel'))\n",
    "\n",
    "channels = ['R', 'S', 'T', 'U', 'V', 'W']\n",
    "row_col = [(1,1), (1,2), (1,3), (2,1), (2,2), (2,3)]\n",
    "\n",
    "for (channel, (row, col)) in zip(channels, row_col):\n",
    "    for medicine in medicines:\n",
    "        med_data = df[df['Medicine_Name'] == medicine]\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=med_data['Dilution_Percent'], \n",
    "                      y=med_data[channel],\n",
    "                      name=f'{medicine} - {channel}',\n",
    "                      mode='markers',\n",
    "                      marker=dict(size=8),\n",
    "                      showlegend=True if row == 1 and col == 1 else False),\n",
    "            row=row, col=col\n",
    "        )\n",
    "\n",
    "fig.update_layout(height=800, width=1200, title_text=\"Sensor Readings vs Dilution by Medicine Type\")\n",
    "fig.update_xaxes(title_text=\"Dilution (%)\")\n",
    "fig.update_yaxes(title_text=\"Sensor Reading\")\n",
    "fig.show()\n",
    "\n",
    "# Create effectiveness heatmap\n",
    "df_pivot = df.pivot_table(values='Effectiveness_Score', \n",
    "                         index='Medicine_Name', \n",
    "                         columns='Dilution_Percent',\n",
    "                         aggfunc='mean')\n",
    "\n",
    "fig_heatmap = go.Figure(data=go.Heatmap(\n",
    "    z=df_pivot.values,\n",
    "    x=df_pivot.columns,\n",
    "    y=df_pivot.index,\n",
    "    colorscale='RdBu',\n",
    "    colorbar=dict(title='Effectiveness Score')\n",
    "))\n",
    "\n",
    "fig_heatmap.update_layout(\n",
    "    title='Effectiveness Score by Medicine and Dilution',\n",
    "    xaxis_title='Dilution (%)',\n",
    "    yaxis_title='Medicine',\n",
    "    width=1000,\n",
    "    height=500\n",
    ")\n",
    "fig_heatmap.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9595e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Twarita Singh\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\core\\dense.py:92: UserWarning:\n",
      "\n",
      "Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "\n",
      "C:\\Users\\Twarita Singh\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\core\\dense.py:92: UserWarning:\n",
      "\n",
      "Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "\n",
      "C:\\Users\\Twarita Singh\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\core\\dense.py:92: UserWarning:\n",
      "\n",
      "Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "\n",
      "C:\\Users\\Twarita Singh\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\core\\dense.py:92: UserWarning:\n",
      "\n",
      "Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "\n",
      "C:\\Users\\Twarita Singh\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\core\\dense.py:92: UserWarning:\n",
      "\n",
      "Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ForestRegressor.predict() got an unexpected keyword argument 'return_std'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m      6\u001b[39m cv_scores = model.train(\n\u001b[32m      7\u001b[39m     train_data[\u001b[33m'\u001b[39m\u001b[33mX\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m      8\u001b[39m     train_data[\u001b[33m'\u001b[39m\u001b[33my_dilution\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m      9\u001b[39m     train_data[\u001b[33m'\u001b[39m\u001b[33my_medicine\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     10\u001b[39m     train_data[\u001b[33m'\u001b[39m\u001b[33my_effectiveness\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     11\u001b[39m )\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Generate predictions for test data\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m test_predictions = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Create true values dictionary for visualization\u001b[39;00m\n\u001b[32m     17\u001b[39m true_values = {\n\u001b[32m     18\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mdilution\u001b[39m\u001b[33m'\u001b[39m: test_data[\u001b[33m'\u001b[39m\u001b[33my_dilution\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     19\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmedicine\u001b[39m\u001b[33m'\u001b[39m: test_data[\u001b[33m'\u001b[39m\u001b[33my_medicine\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     20\u001b[39m     \u001b[33m'\u001b[39m\u001b[33meffectiveness\u001b[39m\u001b[33m'\u001b[39m: test_data[\u001b[33m'\u001b[39m\u001b[33my_effectiveness\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     21\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Twarita Singh\\OneDrive\\Desktop\\Etongue\\ayurvedic-ml-pipeline\\notebooks\\..\\src\\models\\ensemble.py:169\u001b[39m, in \u001b[36mEnsembleModel.predict\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    167\u001b[39m \u001b[38;5;66;03m# Use model-specific confidence estimation\u001b[39;00m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_name == \u001b[33m'\u001b[39m\u001b[33mrf\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m     conf = \u001b[32m1.0\u001b[39m - \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_std\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    171\u001b[39m     conf = np.ones_like(pred)  \u001b[38;5;66;03m# placeholder for SVM\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: ForestRegressor.predict() got an unexpected keyword argument 'return_std'"
     ]
    }
   ],
   "source": [
    "# Visualize model predictions and performance\n",
    "import plotly.express as px\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error, r2_score\n",
    "\n",
    "# Dilution Model Performance\n",
    "y_pred_dilution = cv_scores['dilution']['predictions']\n",
    "y_true_dilution = cv_scores['dilution']['true_values']\n",
    "\n",
    "fig_dilution = px.scatter(\n",
    "    x=y_true_dilution, \n",
    "    y=y_pred_dilution,\n",
    "    labels={'x': 'True Dilution (%)', 'y': 'Predicted Dilution (%)'},\n",
    "    title=f'Dilution Predictions (R² = {r2_score(y_true_dilution, y_pred_dilution):.3f})'\n",
    ")\n",
    "fig_dilution.add_trace(\n",
    "    go.Scatter(x=[0, 100], y=[0, 100], mode='lines', \n",
    "               name='Perfect Prediction', line=dict(dash='dash'))\n",
    ")\n",
    "fig_dilution.show()\n",
    "\n",
    "# Medicine Classification Performance\n",
    "y_pred_medicine = cv_scores['medicine']['predictions']\n",
    "y_true_medicine = cv_scores['medicine']['true_values']\n",
    "\n",
    "conf_matrix = confusion_matrix(y_true_medicine, y_pred_medicine)\n",
    "fig_conf = px.imshow(conf_matrix,\n",
    "                     labels=dict(x=\"Predicted Medicine\", y=\"True Medicine\"),\n",
    "                     x=medicines,\n",
    "                     y=medicines,\n",
    "                     title=\"Medicine Classification Confusion Matrix\",\n",
    "                     color_continuous_scale='Blues')\n",
    "fig_conf.show()\n",
    "\n",
    "# Effectiveness Model Performance\n",
    "y_pred_effectiveness = cv_scores['effectiveness']['predictions']\n",
    "y_true_effectiveness = cv_scores['effectiveness']['true_values']\n",
    "\n",
    "fig_effectiveness = px.scatter(\n",
    "    x=y_true_effectiveness, \n",
    "    y=y_pred_effectiveness,\n",
    "    labels={'x': 'True Effectiveness', 'y': 'Predicted Effectiveness'},\n",
    "    title=f'Effectiveness Predictions (R² = {r2_score(y_true_effectiveness, y_pred_effectiveness):.3f})'\n",
    ")\n",
    "fig_effectiveness.add_trace(\n",
    "    go.Scatter(x=[0, 1], y=[0, 1], mode='lines', \n",
    "               name='Perfect Prediction', line=dict(dash='dash'))\n",
    ")\n",
    "fig_effectiveness.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0c28af",
   "metadata": {},
   "source": [
    "# Real-time Prediction Example\n",
    "\n",
    "Finally, let's demonstrate how to use the API for real-time predictions:\n",
    "1. Create a sample sensor reading\n",
    "2. Send it to the prediction endpoint\n",
    "3. Visualize the results with confidence intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e13e7089",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 39 features, but RandomForestRegressor is expecting 7 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     29\u001b[39m sample_features = feature_engineer.engineer_features(\n\u001b[32m     30\u001b[39m     sample_X_norm[:, :-\u001b[32m1\u001b[39m],\n\u001b[32m     31\u001b[39m     sample_X_norm[:, -\u001b[32m1\u001b[39m],\n\u001b[32m     32\u001b[39m     fit=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     33\u001b[39m )\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Generate predictions\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m sample_predictions = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_features\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcombined\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Display results\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPrediction Results:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Twarita Singh\\OneDrive\\Desktop\\Etongue\\ayurvedic-ml-pipeline\\notebooks\\..\\src\\models\\ensemble.py:166\u001b[39m, in \u001b[36mEnsembleModel.predict\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    165\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_name != \u001b[33m'\u001b[39m\u001b[33mnn\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m         pred = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    167\u001b[39m         \u001b[38;5;66;03m# Use model-specific confidence estimation\u001b[39;00m\n\u001b[32m    168\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m model_name == \u001b[33m'\u001b[39m\u001b[33mrf\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\ensemble\\_forest.py:1065\u001b[39m, in \u001b[36mForestRegressor.predict\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m   1063\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m   1064\u001b[39m \u001b[38;5;66;03m# Check data\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1065\u001b[39m X = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_X_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1067\u001b[39m \u001b[38;5;66;03m# Assign chunk of trees to jobs\u001b[39;00m\n\u001b[32m   1068\u001b[39m n_jobs, _, _ = _partition_estimators(\u001b[38;5;28mself\u001b[39m.n_estimators, \u001b[38;5;28mself\u001b[39m.n_jobs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\ensemble\\_forest.py:637\u001b[39m, in \u001b[36mBaseForest._validate_X_predict\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    634\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    635\u001b[39m     ensure_all_finite = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m637\u001b[39m X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    638\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    639\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    640\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    641\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    642\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    643\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    644\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m issparse(X) \u001b[38;5;129;01mand\u001b[39;00m (X.indices.dtype != np.intc \u001b[38;5;129;01mor\u001b[39;00m X.indptr.dtype != np.intc):\n\u001b[32m    646\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py:2975\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2972\u001b[39m     out = X, y\n\u001b[32m   2974\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params.get(\u001b[33m\"\u001b[39m\u001b[33mensure_2d\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m2975\u001b[39m     \u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2977\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py:2839\u001b[39m, in \u001b[36m_check_n_features\u001b[39m\u001b[34m(estimator, X, reset)\u001b[39m\n\u001b[32m   2836\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   2838\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_features != estimator.n_features_in_:\n\u001b[32m-> \u001b[39m\u001b[32m2839\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2840\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2841\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator.n_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m features as input.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2842\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: X has 39 features, but RandomForestRegressor is expecting 7 features as input."
     ]
    }
   ],
   "source": [
    "# Create a sample reading\n",
    "sample_reading = {\n",
    "    'R': 3.5,\n",
    "    'S': 2.8,\n",
    "    'T': 4.2,\n",
    "    'U': 3.0,\n",
    "    'V': 3.8,\n",
    "    'W': 2.5,\n",
    "    'Temperature': 25.0\n",
    "}\n",
    "\n",
    "# Convert to numpy array\n",
    "sample_X = np.array([[\n",
    "    sample_reading['R'],\n",
    "    sample_reading['S'],\n",
    "    sample_reading['T'],\n",
    "    sample_reading['U'],\n",
    "    sample_reading['V'],\n",
    "    sample_reading['W'],\n",
    "    sample_reading['Temperature']\n",
    "]])\n",
    "\n",
    "# Preprocess the sample\n",
    "sample_X_norm = preprocessor.normalize_features(\n",
    "    preprocessor.temperature_compensation(pd.DataFrame([sample_reading]))\n",
    ").values\n",
    "\n",
    "# Engineer features\n",
    "sample_features = feature_engineer.engineer_features(\n",
    "    sample_X_norm[:, :-1],\n",
    "    sample_X_norm[:, -1],\n",
    "    fit=False\n",
    ")\n",
    "\n",
    "# Generate predictions\n",
    "sample_predictions = model.predict(sample_features['combined'])\n",
    "\n",
    "# Display results\n",
    "print(\"Prediction Results:\")\n",
    "print(f\"Dilution: {sample_predictions['dilution']['predictions'][0]:.1f}% (confidence: {sample_predictions['dilution']['confidence'][0]:.2f})\")\n",
    "print(f\"Medicine: {sample_predictions['medicine']['predictions'][0]} (confidence: {sample_predictions['medicine']['confidence'][0]:.2f})\")\n",
    "print(f\"Effectiveness: {sample_predictions['effectiveness']['predictions'][0]:.2f} (confidence: {sample_predictions['effectiveness']['confidence'][0]:.2f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
